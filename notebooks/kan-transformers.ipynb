{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7419eafc-5a18-4929-9d78-85dff0e86b4a",
   "metadata": {},
   "source": [
    "# Install Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48591c90-7fab-482c-83b6-fce90a7d2afa",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2401e2c-eb58-46b9-9bab-13f36db6766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import torch\n",
    "from torch import (\n",
    "    nn, \n",
    "    Tensor, \n",
    "    FloatTensor, \n",
    "    LongTensor\n",
    ")\n",
    "from kan import KANLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ElectraGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_type_size: int,\n",
    "        layernorm_eps: float,\n",
    "        embedding_dropout_p: float,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_layers: int,\n",
    "        max_pos_embedding: int = 512,\n",
    "    ) -> None:\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.embedding_layer = Embedding(vocab_size, embedding_dim, max_pos_embedding, vocab_type_size, layernorm_eps, embedding_dropout_p)\n",
    "        self.head = GeneratorHead(hidden_dim, embedding_dim, vocab_size, layernorm_eps)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: LongTensor,\n",
    "        attention_mask: Optional[LongTensor] = None,\n",
    "        token_type_ids: Optional[LongTensor] = None\n",
    "    ):\n",
    "        if not attention_mask:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if not token_type_ids:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        hidden_states = self.embedding_layer(input_ids, attention_mask, token_type_ids)\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        return self.head(hidden_states)\n",
    "\n",
    "\n",
    "class ElectraDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        vocab_type_size: int,\n",
    "        layernorm_eps: float,\n",
    "        embedding_dropout_p: float,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        num_layers: int,\n",
    "        max_pos_embedding: int = 512,\n",
    "        num_labels: int = 1 \n",
    "    ):\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.embedding_layer = Embedding(vocab_size, embedding_dim, max_pos_embedding, vocab_type_size, layernorm_eps, embedding_dropout_p)\n",
    "        self.head = Classifier(hidden_dim, embedding_dim, num_labels, layernorm_eps)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: LongTensor,\n",
    "        attention_mask: Optional[LongTensor] = None,\n",
    "        token_type_ids: Optional[LongTensor] = None\n",
    "    ):\n",
    "        if not attention_mask:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if not token_type_ids:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        hidden_states = self.embedding_layer(input_ids, attention_mask, token_type_ids)\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        return self.head(hidden_states)\n",
    "    \n",
    "\n",
    "class GeneratorHead(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, embedding_dim: int, vocab_size: int, eps: float) -> None:\n",
    "        super().__init__()\n",
    "        self.kan = KANLayer(hidden_dim, embedding_dim)\n",
    "        self.out = KANLayer(embedding_dim, vocab_size)\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, hidden: FloatTensor) -> FloatTensor:\n",
    "        hidden = self.kan(hidden)\n",
    "        hidden = F.gelu(hidden)\n",
    "        hidden = F.layer_norm(hidden, eps=self.eps)\n",
    "        return self.out(hidden)\n",
    "        \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_labels: int,\n",
    "    ):\n",
    "        self.kan = KANLayer(hidden_dim, hidden_dim)\n",
    "        self.out = KANLayer(hidden_dim, num_labels)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden: FloatTensor\n",
    "    ):\n",
    "        hidden = self.kan(hidden)\n",
    "        hidden = F.gelu(hidden)\n",
    "        return self.out(hidden).squeeze(-1)\n",
    "\n",
    "\n",
    "class ElectraEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        max_len: int\n",
    "    ) -> None:\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.input_ids_embedding = PositionalEncoding(dim, max_len)\n",
    "        self.pos_embedding = PositionalEncoding(dim, max_len)\n",
    "        self.token_type_ids_embedding = PositionalEncoding(dim, max_len)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: LongTensor,\n",
    "        attention_mask: Optional[LongTensor] = None,\n",
    "        token_type_ids: Optional[LongTensor] = None\n",
    "    ):\n",
    "        if not attention_mask:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if not token_type_ids:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        hidden_states = self.pos_enc(input_ids)\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, ff_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(dim, num_heads)\n",
    "        self.ff = PositionWideFeedForward(dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: FloatTensor, \n",
    "        mask: Optional[Tensor] = None\n",
    "        ) -> FloatTensor:\n",
    "        x = self.attn(x, x, x, mask) + x\n",
    "        x = self.norm1(x)\n",
    "        x = self.ff(x) + x\n",
    "        return self.norm2(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.query = KANLayer(dim, dim)\n",
    "        self.key = KANLayer(dim, dim)\n",
    "        self.value = KANLayer(dim, dim)\n",
    "        self.out = KANLayer(dim, dim)\n",
    "        \n",
    "    def scaled_dot_production_attn(self, query: FloatTensor, key: FloatTensor, value: FloatTensor, mask: Optional[Tensor] = None) -> Tuple[FloatTensor, FloatTensor]:\n",
    "        scores = query @ key.transpose(-2, -1) * self.scale\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        return attn @ value, attn   # Thanks copilot!\n",
    "    \n",
    "    def split_heads(self, x: FloatTensor) -> Tensor:\n",
    "        return x.view(x.size(0), x.size(1), self.num_heads, self.head_dim).transpose(1, 2)  # Thanks copilot! - 2\n",
    "    \n",
    "    def combine_heads(self, x: FloatTensor) -> Tensor:\n",
    "        return x.transpose(1, 2).contiguous().view(x.size(0), x.size(1), self.dim)  # Thanks copilot!\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        query: FloatTensor, \n",
    "        key: FloatTensor, \n",
    "        value: FloatTensor, \n",
    "        mask: Optional[Tensor] = None\n",
    "        ) -> FloatTensor:\n",
    "        query = self.split_heads(self.query(query))\n",
    "        key = self.split_heads(self.key(key))\n",
    "        value = self.split_heads(self.value(value))\n",
    "        \n",
    "        x, attn = self.scaled_dot_production_attn(query, key, value, mask)\n",
    "        \n",
    "        return self.out(self.combine_heads(x))\n",
    "\n",
    "\n",
    "class PositionWideFeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, intermediate_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = KANLayer(dim, intermediate_dim)\n",
    "        self.fc2 = KANLayer(intermediate_dim, dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: FloatTensor) -> FloatTensor:\n",
    "        return self.fc2(self.activation(self.fc1(x)))\n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim: int, max_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self.pos_enc = nn.Parameter(torch.zeros(max_len, dim))\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, dim, 2) * -(torch.log(torch.tensor(10000.0)) / dim))\n",
    "        self.pos_enc[:, 0::2] = torch.sin(pos * div)\n",
    "        self.pos_enc[:, 1::2] = torch.cos(pos * div)\n",
    "        \n",
    "        self.register_buffer('pos_enc', self.pos_enc)\n",
    "        \n",
    "    def forward(self, x: FloatTensor) -> FloatTensor:\n",
    "        return x + self.pos_enc[:x.size(1)]\n",
    "    \n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int, \n",
    "        embedding_dim: int, \n",
    "        max_pos_embedding: int, \n",
    "        vocab_type_size: Optional[int] = 2,\n",
    "        eps: Optional[float] = 1e-12,\n",
    "        dropout_p: Optional[float] = .1,\n",
    "        positional_embedding_type: str = 'absolute'\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)  \n",
    "        self.pos_embedding = nn.Embedding(max_pos_embedding, embedding_dim)\n",
    "        self.token_type_embedding = nn.Embedding(vocab_type_size, embedding_dim)\n",
    "        \n",
    "        self.layernorm = nn.LayerNorm(embedding_dim, eps=eps)\n",
    "        self.dropout_p = dropout_p\n",
    "        self.positional_embedding_type = positional_embedding_type\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: LongTensor,\n",
    "        attention_mask: LongTensor,\n",
    "        token_type_ids: LongTensor,\n",
    "    ) -> FloatTensor:\n",
    "        input_embedding = self.word_embedding(input_ids)\n",
    "        token_type_embedding = self.token_type_embedding(attention_mask)\n",
    "        embedding = input_embedding + token_type_embedding\n",
    "        if self.positional_embedding_type in ['absolute', 'abs']:\n",
    "            pos_embedding = self.pos_embedding(attention_mask) \n",
    "            embedding += pos_embedding\n",
    "        embedding = self.layernorm(embedding)\n",
    "        embedding = F.dropout(embedding, p=self.dropout_p)\n",
    "        return embedding        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae8c87-7960-44c7-86e9-dc1c8b6ebdd5",
   "metadata": {},
   "source": [
    "## Modeling test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f8155f-343c-43f2-b451-4572ac1f8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, ElectraForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained('google/electra-base-generator')\n",
    "generator_config = AutoConfig.from_pretrained('google/electra-base-generator')\n",
    "\n",
    "\n",
    "discriminator_tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "discriminator_config = AutoConfig.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "\n",
    "random_input_ids = torch.randint(0, len(generator_tokenizer), (1, 512))\n",
    "random_attention_mask = torch.randint(0, 1, (1, 512))\n",
    "random_token_type_ids = torch.randint(0, 1, (1, 512))\n",
    "\n",
    "orig_generator = ElectraForMaskedLM(generator_config)\n",
    "generator = ElectraGenerator(\n",
    "    vocab_size=len(generator_tokenizer),\n",
    "    embedding_dim=768,\n",
    "    vocab_type_size=2,\n",
    "    layernorm_eps=1e-12,\n",
    "    embedding_dropout_p=.1,\n",
    "    hidden_dim=768,\n",
    "    num_heads=12,\n",
    "    ff_dim=3072,\n",
    "    num_layers=12,\n",
    "    max_pos_embedding=512\n",
    ") \n",
    "\n",
    "\n",
    "print(\"initialzation complete.\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    orig_generator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    orig_output = orig_generator(input_ids=random_input_ids, attention_mask=random_attention_mask, token_type_ids=random_token_type_ids)\n",
    "    output = generator(input_ids=random_input_ids, attention_mask=random_attention_mask, token_type_ids=random_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bb447-505d-469f-b00a-072d569da746",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(orig_output, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c7aa7-c508-49ea-bfe2-36c6eea5b69c",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148d65f-9294-4ed6-adfd-29099f4c536d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89080ba-05e7-479a-8ba8-16f030ef7665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
